# polls/views.py
import os
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForSequenceClassification
from django.shortcuts import render
from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
from django.core.files.storage import default_storage
from .models import Category, Up_Fle
from .forms import UploadFileForm
import numpy as np
from tqdm import tqdm
import tempfile
from django.core.files import File
from io import StringIO
from django.core.files.base import ContentFile
from django.core.files.storage import default_storage
from django.core.cache import cache
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import re
import time

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
MAX_LENGTH = 150
SCALING_FACTOR = 0.7
PROGRESS_KEY = 'file_processing_progress'

def set_progress(percent):
    cache.set(PROGRESS_KEY, percent, timeout=600)  # –•—Ä–∞–Ω–∏–º –¥–æ 10 –º–∏–Ω—É—Ç

def check_model_files():
    model_dir = '/workspaces/Pre-diploma-internship/pre-processing_service/polls/best_model'
    required_files = ['config.json', 'vocab.txt', 'model.safetensors', 'tokenizer_config.json', 'special_tokens_map.json']
    missing_files = [f for f in required_files if not os.path.exists(os.path.join(model_dir, f))]
    if missing_files:
        raise FileNotFoundError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏: {', '.join(missing_files)}")
    print("‚úÖ –í—Å–µ —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –Ω–∞–π–¥–µ–Ω—ã")

class PredictionDataset(Dataset):
    def __init__(self, texts, tokenizer):
        self.texts = texts
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        if not isinstance(text, str):
            text = str(text)
        encoding = self.tokenizer(
            text,
            padding="max_length",
            truncation=True,
            max_length=MAX_LENGTH,
            return_tensors="pt"
        )
        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
        }

def load_model_and_tokenizer():
    try:
        check_model_files()
        tokenizer = BertTokenizer.from_pretrained('/workspaces/Pre-diploma-internship/pre-processing_service/polls/best_model')
        model = BertForSequenceClassification.from_pretrained('/workspaces/Pre-diploma-internship/pre-processing_service/polls/best_model')
        return tokenizer, model
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {str(e)}. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—Å–µ —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –ø–∞–ø–∫–µ best_model.")
        raise

def generate_graphs(output_df):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–≤–∞ –≥—Ä–∞—Ñ–∏–∫–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Ö URL-–∞–¥—Ä–µ—Å–∞.
    """
    static_dir = '/workspaces/Pre-diploma-internship/pre-processing_service/polls/static/images'
    
    
    labels_path = os.path.join(os.path.dirname(__file__), '–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.xlsx')
    labels_df = pd.read_excel(labels_path)
    required_columns = ['level_3', 'level_2']
    label_to_category = dict(zip(labels_df['level_3'], labels_df['level_2']))
    
    def map_to_category(labels_str):
        if not isinstance(labels_str, str) or not labels_str.strip():
            return []
        labels = [label.strip() for label in labels_str.split(";")]
        categories = set(label_to_category[label] for label in labels if label in label_to_category)
        return list(categories)

    output_df['categories'] = output_df['predicted_labels'].apply(map_to_category)
    
    
    
    # –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã
    exploded_categories = output_df.explode('categories')
    category_counts = exploded_categories['categories'].value_counts().sort_values(ascending=False)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ —É–±—ã–≤–∞–Ω–∏—é
    sorted_categories = category_counts.sort_values(ascending=True)
    
    # –¶–≤–µ—Ç–æ–≤–∞—è –≥–∞–º–º–∞
    colors = ['#fd7e14', '#6c757d', '#17a2b8', '#dc3545', '#ffc107', '#28a745', '#007bff']
    
    fig, ax = plt.subplots(figsize=(10, 6), facecolor='none')  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ–∑—Ä–∞—á–Ω—ã–π —Ñ–æ–Ω –¥–ª—è –≤—Å–µ–π —Ñ–∏–≥—É—Ä—ã
    
    # –†–∏—Å—É–µ–º –±–∞—Ä—ã —Å zorder=2 (–æ–Ω–∏ –±—É–¥—É—Ç –≤—ã—à–µ —Å–µ—Ç–∫–∏)
    bars = ax.barh(sorted_categories.index, sorted_categories.values,
                   color=colors[:len(sorted_categories)], zorder=2, height=0.5)
    
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Å–µ—Ç–∫—É —Å zorder=1 (–æ–Ω–∞ –±—É–¥–µ—Ç –ø–æ–¥ –±–∞—Ä–∞–º–∏)
    ax.grid(axis='x', color='gray', linestyle='--', linewidth=0.7, zorder=1)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    ax.tick_params(axis='both', colors='black')
    
    # –ü—Ä–æ–∑—Ä–∞—á–Ω—ã–π —Ñ–æ–Ω
    ax.set_facecolor('none')
    
    graph1_path = os.path.join(static_dir, 'graph1.png')
    
    plt.savefig(str(graph1_path), transparent=True, bbox_inches='tight', pad_inches=0.1)
    plt.close()
    
    # –ì—Ä–∞—Ñ–∏–∫ 2: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è–º (—Ç–æ–ø-10)
    subcategory_counts = output_df['predicted_labels'].str.split('; ').explode().value_counts().head(10)
    
    # –¶–≤–µ—Ç –≤—Å–µ—Ö –±–∞—Ä–æ–≤
    bar_color = '#007bff'
    
    # –†–∏—Å—É–µ–º –≥—Ä–∞—Ñ–∏–∫
    fig, ax = plt.subplots(figsize=(10, 6), facecolor='none')

    bars = ax.barh(subcategory_counts.index[::-1], subcategory_counts.values[::-1],
                   color=bar_color, zorder=2, height=0.5)
                   
                   

    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Å–µ—Ç–∫—É
    ax.grid(axis='x', color='gray', linestyle='--', linewidth=0.7, zorder=1)
    
    # –¢–æ–ª—å–∫–æ —Ü–µ–ª—ã–µ —á–∏—Å–ª–∞ –Ω–∞ –æ—Å–∏ X
    from matplotlib.ticker import MaxNLocator
    ax.xaxis.set_major_locator(MaxNLocator(integer=True))

    ax.tick_params(axis='both', colors='black')

    # –ü—Ä–æ–∑—Ä–∞—á–Ω—ã–π —Ñ–æ–Ω
    ax.set_facecolor('none')
    
    graph2_path = os.path.join(static_dir, 'graph2.png')
    plt.savefig(str(graph2_path), transparent=True, bbox_inches='tight', pad_inches=0.1)
    plt.close()
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º URL-–∞–¥—Ä–µ—Å–∞ –≥—Ä–∞—Ñ–∏–∫–æ–≤
    
    # üî• –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –º–µ—Ç–∫—É –∫ URL, —á—Ç–æ–±—ã –æ—Ç–∫–ª—é—á–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
    timestamp = int(time.time())
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º URL —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º v={timestamp}
    graph1_url = '/static/images/graph1.png?v=' + str(timestamp)
    graph2_url = '/static/images/graph2.png?v=' + str(timestamp)
    
    return graph1_url, graph2_url


def generate_wordcloud(output_df):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–±–ª–∞–∫–æ —Å–ª–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ DataFrame,
    –∏—Å–∫–ª—é—á–∞—è –ø—Ä–µ–¥–ª–æ–≥–∏ –∏ —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–ª–æ–≤–∞.
    """
    # –°–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤
    STOP_WORDS = {
        'c', '–∞', '–∞–ª–ª–æ', '–±–µ–∑', '–±–µ–ª—ã–π', '–±–ª–∏–∑–∫–æ', '–±–æ–ª–µ–µ', '–±–æ–ª—å—à–µ',
        '–±–æ–ª—å—à–æ–π', '–±—É–¥–µ–º', '–±—É–¥–µ—Ç', '–±—É–¥–µ—Ç–µ', '–±—É–¥–µ—à—å', '–±—É–¥—Ç–æ', '–±—É–¥—É',
        '–±—É–¥—É—Ç', '–±—É–¥—å', '–±—ã', '–±—ã–≤–∞–µ—Ç', '–±—ã–≤—å', '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–∏',
        '–±—ã–ª–æ', '–±—ã—Ç—å', '–≤', '–≤–∞–∂–Ω–∞—è', '–≤–∞–∂–Ω–æ–µ', '–≤–∞–∂–Ω—ã–µ', '–≤–∞–∂–Ω—ã–π', '–≤–∞–º',
        '–≤–∞–º–∏', '–≤–∞—Å', '–≤–∞—à', '–≤–∞—à–∞', '–≤–∞—à–µ', '–≤–∞—à–∏', '–≤–≤–µ—Ä—Ö', '–≤–¥–∞–ª–∏',
        '–≤–¥—Ä—É–≥', '–≤–µ–¥—å', '–≤–µ–∑–¥–µ', '–≤–µ—Ä–Ω—É—Ç—å—Å—è', '–≤–µ—Å—å', '–≤–µ—á–µ—Ä', '–≤–∑–≥–ª—è–¥',
        '–≤–∑—è—Ç—å', '–≤–∏–¥', '–≤–∏–¥–µ–ª', '–≤–∏–¥–µ—Ç—å', '–≤–º–µ—Å—Ç–µ', '–≤–Ω–µ', '–≤–Ω–∏–∑', '–≤–Ω–∏–∑—É',
        '–≤–æ', '–≤–æ–¥–∞', '–≤–æ–π–Ω–∞', '–≤–æ–∫—Ä—É–≥', '–≤–æ–Ω', '–≤–æ–æ–±—â–µ', '–≤–æ–ø—Ä–æ—Å',
        '–≤–æ—Å–µ–º–Ω–∞–¥—Ü–∞—Ç—ã–π', '–≤–æ—Å–µ–º–Ω–∞–¥—Ü–∞—Ç—å', '–≤–æ—Å–µ–º—å', '–≤–æ—Å—å–º–æ–π', '–≤–æ—Ç',
        '–≤–ø—Ä–æ—á–µ–º', '–≤—Ä–µ–º–µ–Ω–∏', '–≤—Ä–µ–º—è', '–≤—Å–µ', '–≤—Å–µ –µ—â–µ', '–≤—Å–µ–≥–¥–∞', '–≤—Å–µ–≥–æ',
        '–≤—Å–µ–º', '–≤—Å–µ–º–∏', '–≤—Å–µ–º—É', '–≤—Å–µ—Ö', '–≤—Å–µ—é', '–≤—Å—é', '–≤—Å—é–¥—É', '–≤—Å—è',
        '–≤—Å—ë', '–≤—Ç–æ—Ä–æ–π', '–≤—ã', '–≤—ã–π—Ç–∏', '–≥', '–≥–¥–µ', '–≥–ª–∞–≤–Ω—ã–π', '–≥–ª–∞–∑',
        '–≥–æ–≤–æ—Ä–∏–ª', '–≥–æ–≤–æ—Ä–∏—Ç', '–≥–æ–≤–æ—Ä–∏—Ç—å', '–≥–æ–¥', '–≥–æ–¥–∞', '–≥–æ–¥—É', '–≥–æ–ª–æ–≤–∞',
        '–≥–æ–ª–æ—Å', '–≥–æ—Ä–æ–¥', '–¥–∞', '–¥–∞–≤–∞—Ç—å', '–¥–∞–≤–Ω–æ', '–¥–∞–∂–µ', '–¥–∞–ª–µ–∫–∏–π',
        '–¥–∞–ª–µ–∫–æ', '–¥–∞–ª—å—à–µ', '–¥–∞—Ä–æ–º', '–¥–∞—Ç—å', '–¥–≤–∞', '–¥–≤–∞–¥—Ü–∞—Ç—ã–π', '–¥–≤–∞–¥—Ü–∞—Ç—å',
        '–¥–≤–µ', '–¥–≤–µ–Ω–∞–¥—Ü–∞—Ç—ã–π', '–¥–≤–µ–Ω–∞–¥—Ü–∞—Ç—å', '–¥–≤–µ—Ä—å', '–¥–≤—É—Ö', '–¥–µ–≤—è—Ç–Ω–∞–¥—Ü–∞—Ç—ã–π',
        '–¥–µ–≤—è—Ç–Ω–∞–¥—Ü–∞—Ç—å', '–¥–µ–≤—è—Ç—ã–π', '–¥–µ–≤—è—Ç—å', '–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ', '–¥–µ–ª', '–¥–µ–ª–∞–ª',
        '–¥–µ–ª–∞—Ç—å', '–¥–µ–ª–∞—é', '–¥–µ–ª–æ', '–¥–µ–Ω—å', '–¥–µ–Ω—å–≥–∏', '–¥–µ—Å—è—Ç—ã–π', '–¥–µ—Å—è—Ç—å',
        '–¥–ª—è', '–¥–æ', '–¥–æ–≤–æ–ª—å–Ω–æ', '–¥–æ–ª–≥–æ', '–¥–æ–ª–∂–µ–Ω', '–¥–æ–ª–∂–Ω–æ', '–¥–æ–ª–∂–Ω—ã–π',
        '–¥–æ–º', '–¥–æ—Ä–æ–≥–∞', '–¥—Ä—É–≥', '–¥—Ä—É–≥–∞—è', '–¥—Ä—É–≥–∏–µ', '–¥—Ä—É–≥–∏—Ö', '–¥—Ä—É–≥–æ',
        '–¥—Ä—É–≥–æ–µ', '–¥—Ä—É–≥–æ–π', '–¥—É–º–∞—Ç—å', '–¥—É—à–∞', '–µ', '–µ–≥–æ', '–µ–µ', '–µ–π', '–µ–º—É',
        '–µ—Å–ª–∏', '–µ—Å—Ç—å', '–µ—â–µ', '–µ—â—ë', '–µ—é', '–µ—ë', '–∂', '–∂–¥–∞—Ç—å', '–∂–µ', '–∂–µ–Ω–∞',
        '–∂–µ–Ω—â–∏–Ω–∞', '–∂–∏–∑–Ω—å', '–∂–∏—Ç—å', '–∑–∞', '–∑–∞–Ω—è—Ç', '–∑–∞–Ω—è—Ç–∞', '–∑–∞–Ω—è—Ç–æ',
        '–∑–∞–Ω—è—Ç—ã', '–∑–∞—Ç–µ–º', '–∑–∞—Ç–æ', '–∑–∞—á–µ–º', '–∑–¥–µ—Å—å', '–∑–µ–º–ª—è', '–∑–Ω–∞—Ç—å',
        '–∑–Ω–∞—á–∏—Ç', '–∑–Ω–∞—á–∏—Ç—å', '–∏', '–∏–¥–∏', '–∏–¥—Ç–∏', '–∏–∑', '–∏–ª–∏', '–∏–º', '–∏–º–µ–ª',
        '–∏–º–µ–Ω–Ω–æ', '–∏–º–µ—Ç—å', '–∏–º–∏', '–∏–º—è', '–∏–Ω–æ–≥–¥–∞', '–∏—Ö', '–∫', '–∫–∞–∂–¥–∞—è',
        '–∫–∞–∂–¥–æ–µ', '–∫–∞–∂–¥—ã–µ', '–∫–∞–∂–¥—ã–π', '–∫–∞–∂–µ—Ç—Å—è', '–∫–∞–∑–∞—Ç—å—Å—è', '–∫–∞–∫', '–∫–∞–∫–∞—è',
        '–∫–∞–∫–æ–π', '–∫–µ–º', '–∫–Ω–∏–≥–∞', '–∫–æ–≥–¥–∞', '–∫–æ–≥–æ', '–∫–æ–º', '–∫–æ–º–Ω–∞—Ç–∞', '–∫–æ–º—É',
        '–∫–æ–Ω–µ—Ü', '–∫–æ–Ω–µ—á–Ω–æ', '–∫–æ—Ç–æ—Ä–∞—è', '–∫–æ—Ç–æ—Ä–æ–≥–æ', '–∫–æ—Ç–æ—Ä–æ–π', '–∫–æ—Ç–æ—Ä—ã–µ',
        '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä—ã—Ö', '–∫—Ä–æ–º–µ', '–∫—Ä—É–≥–æ–º', '–∫—Ç–æ', '–∫—É–¥–∞', '–ª–µ–∂–∞—Ç—å',
        '–ª–µ—Ç', '–ª–∏', '–ª–∏—Ü–æ', '–ª–∏—à—å', '–ª—É—á—à–µ', '–ª—é–±–∏—Ç—å', '–ª—é–¥–∏', '–º',
        '–º–∞–ª–µ–Ω—å–∫–∏–π', '–º–∞–ª–æ', '–º–∞—Ç—å', '–º–∞—à–∏–Ω–∞', '–º–µ–∂–¥—É', '–º–µ–ª—è', '–º–µ–Ω–µ–µ',
        '–º–µ–Ω—å—à–µ', '–º–µ–Ω—è', '–º–µ—Å—Ç–æ', '–º–∏–ª–ª–∏–æ–Ω–æ–≤', '–º–∏–º–æ', '–º–∏–Ω—É—Ç–∞', '–º–∏—Ä',
        '–º–∏—Ä–∞', '–º–Ω–µ', '–º–Ω–æ–≥–æ', '–º–Ω–æ–≥–æ—á–∏—Å–ª–µ–Ω–Ω–∞—è', '–º–Ω–æ–≥–æ—á–∏—Å–ª–µ–Ω–Ω–æ–µ',
        '–º–Ω–æ–≥–æ—á–∏—Å–ª–µ–Ω–Ω—ã–µ', '–º–Ω–æ–≥–æ—á–∏—Å–ª–µ–Ω–Ω—ã–π', '–º–Ω–æ–π', '–º–Ω–æ—é', '–º–æ–≥', '–º–æ–≥—É',
        '–º–æ–≥—É—Ç', '–º–æ–∂', '–º–æ–∂–µ—Ç', '–º–æ–∂–µ—Ç –±—ã—Ç—å', '–º–æ–∂–Ω–æ', '–º–æ–∂—Ö–æ', '–º–æ–∏', '–º–æ–π',
        '–º–æ—Ä', '–º–æ—Å–∫–≤–∞', '–º–æ—á—å', '–º–æ—è', '–º–æ—ë', '–º—ã', '–Ω–∞', '–Ω–∞–≤–µ—Ä—Ö—É', '–Ω–∞–¥',
        '–Ω–∞–¥–æ', '–Ω–∞–∑–∞–¥', '–Ω–∞–∏–±–æ–ª–µ–µ', '–Ω–∞–π—Ç–∏', '–Ω–∞–∫–æ–Ω–µ—Ü', '–Ω–∞–º', '–Ω–∞–º–∏',
        '–Ω–∞—Ä–æ–¥', '–Ω–∞—Å', '–Ω–∞—á–∞–ª–∞', '–Ω–∞—á–∞—Ç—å', '–Ω–∞—à', '–Ω–∞—à–∞', '–Ω–∞—à–µ', '–Ω–∞—à–∏',
        '–Ω–µ', '–Ω–µ–≥–æ', '–Ω–µ–¥–∞–≤–Ω–æ', '–Ω–µ–¥–∞–ª–µ–∫–æ', '–Ω–µ–µ', '–Ω–µ–π', '–Ω–µ–∫–æ—Ç–æ—Ä—ã–π',
        '–Ω–µ–ª—å–∑—è', '–Ω–µ–º', '–Ω–µ–º–Ω–æ–≥–æ', '–Ω–µ–º—É', '–Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ', '–Ω–µ—Ä–µ–¥–∫–æ',
        '–Ω–µ—Å–∫–æ–ª—å–∫–æ', '–Ω–µ—Ç', '–Ω–µ—é', '–Ω–µ—ë', '–Ω–∏', '–Ω–∏–±—É–¥—å', '–Ω–∏–∂–µ', '–Ω–∏–∑–∫–æ',
        '–Ω–∏–∫–∞–∫–æ–π', '–Ω–∏–∫–æ–≥–¥–∞', '–Ω–∏–∫—Ç–æ', '–Ω–∏–∫—É–¥–∞', '–Ω–∏–º', '–Ω–∏–º–∏', '–Ω–∏—Ö',
        '–Ω–∏—á–µ–≥–æ', '–Ω–∏—á—Ç–æ', '–Ω–æ', '–Ω–æ–≤—ã–π', '–Ω–æ–≥–∞', '–Ω–æ—á—å', '–Ω—É', '–Ω—É–∂–Ω–æ',
        '–Ω—É–∂–Ω—ã–π', '–Ω—Ö', '–æ', '–æ–±', '–æ–±–∞', '–æ–±—ã—á–Ω–æ', '–æ–¥–∏–Ω', '–æ–¥–∏–Ω–Ω–∞–¥—Ü–∞—Ç—ã–π',
        '–æ–¥–∏–Ω–Ω–∞–¥—Ü–∞—Ç—å', '–æ–¥–Ω–∞–∂–¥—ã', '–æ–¥–Ω–∞–∫–æ', '–æ–¥–Ω–æ–≥–æ', '–æ–¥–Ω–æ–π', '–æ–∫–∞–∑–∞—Ç—å—Å—è',
        '–æ–∫–Ω–æ', '–æ–∫–æ–ª–æ', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–∏', '–æ–Ω–æ', '–æ–ø—è—Ç—å', '–æ—Å–æ–±–µ–Ω–Ω–æ',
        '–æ—Å—Ç–∞—Ç—å—Å—è', '–æ—Ç', '–æ—Ç–≤–µ—Ç–∏—Ç—å', '–æ—Ç–µ—Ü', '–æ—Ç–∫—É–¥–∞', '–æ—Ç–æ–≤—Å—é–¥—É', '–æ—Ç—Å—é–¥–∞',
        '–æ—á–µ–Ω—å', '–ø–µ—Ä–≤—ã–π', '–ø–µ—Ä–µ–¥', '–ø–∏—Å–∞—Ç—å', '–ø–ª–µ—á–æ', '–ø–æ', '–ø–æ–¥', '–ø–æ–¥–æ–π–¥–∏',
        '–ø–æ–¥—É–º–∞—Ç—å', '–ø–æ–∂–∞–ª—É–π—Å—Ç–∞', '–ø–æ–∑–∂–µ', '–ø–æ–π—Ç–∏', '–ø–æ–∫–∞', '–ø–æ–ª', '–ø–æ–ª—É—á–∏—Ç—å',
        '–ø–æ–º–Ω–∏—Ç—å', '–ø–æ–Ω–∏–º–∞—Ç—å', '–ø–æ–Ω—è—Ç—å', '–ø–æ—Ä', '–ø–æ—Ä–∞', '–ø–æ—Å–ª–µ', '–ø–æ—Å–ª–µ–¥–Ω–∏–π',
        '–ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å', '–ø–æ—Å—Ä–µ–¥–∏', '–ø–æ—Ç–æ–º', '–ø–æ—Ç–æ–º—É', '–ø–æ—á–µ–º—É', '–ø–æ—á—Ç–∏',
        '–ø—Ä–∞–≤–¥–∞', '–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ', '–ø—Ä–∏', '–ø—Ä–æ', '–ø—Ä–æ—Å—Ç–æ', '–ø—Ä–æ—Ç–∏–≤', '–ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤',
        '–ø—É—Ç—å', '–ø—è—Ç–Ω–∞–¥—Ü–∞—Ç—ã–π', '–ø—è—Ç–Ω–∞–¥—Ü–∞—Ç—å', '–ø—è—Ç—ã–π', '–ø—è—Ç—å', '—Ä–∞–±–æ—Ç–∞',
        '—Ä–∞–±–æ—Ç–∞—Ç—å', '—Ä–∞–∑', '—Ä–∞–∑–≤–µ', '—Ä–∞–Ω–æ', '—Ä–∞–Ω—å—à–µ', '—Ä–µ–±–µ–Ω–æ–∫', '—Ä–µ—à–∏—Ç—å',
        '—Ä–æ—Å—Å–∏—è', '—Ä—É–∫–∞', '—Ä—É—Å—Å–∫–∏–π', '—Ä—è–¥', '—Ä—è–¥–æ–º', '—Å', '—Å –∫–µ–º', '—Å–∞–º',
        '—Å–∞–º–∞', '—Å–∞–º–∏', '—Å–∞–º–∏–º', '—Å–∞–º–∏–º–∏', '—Å–∞–º–∏—Ö', '—Å–∞–º–æ', '—Å–∞–º–æ–≥–æ',
        '—Å–∞–º–æ–π', '—Å–∞–º–æ–º', '—Å–∞–º–æ–º—É', '—Å–∞–º—É', '—Å–∞–º—ã–π', '—Å–≤–µ—Ç', '—Å–≤–æ–µ', '—Å–≤–æ–µ–≥–æ',
        '—Å–≤–æ–µ–π', '—Å–≤–æ–∏', '—Å–≤–æ–∏—Ö', '—Å–≤–æ–π', '—Å–≤–æ—é', '—Å–¥–µ–ª–∞—Ç—å', '—Å–µ–∞–æ–π', '—Å–µ–±–µ',
        '—Å–µ–±—è', '—Å–µ–≥–æ–¥–Ω—è', '—Å–µ–¥—å–º–æ–π', '—Å–µ–π—á–∞—Å', '—Å–µ–º–Ω–∞–¥—Ü–∞—Ç—ã–π', '—Å–µ–º–Ω–∞–¥—Ü–∞—Ç—å',
        '—Å–µ–º—å', '—Å–∏–¥–µ—Ç—å', '—Å–∏–ª–∞', '—Å–∏—Ö', '—Å–∫–∞–∑–∞–ª', '—Å–∫–∞–∑–∞–ª–∞', '—Å–∫–∞–∑–∞—Ç—å',
        '—Å–∫–æ–ª—å–∫–æ', '—Å–ª–∏—à–∫–æ–º', '—Å–ª–æ–≤–æ', '—Å–ª—É—á–∞–π', '—Å–º–æ—Ç—Ä–µ—Ç—å', '—Å–Ω–∞—á–∞–ª–∞',
        '—Å–Ω–æ–≤–∞', '—Å–æ', '—Å–æ–±–æ–π', '—Å–æ–±–æ—é', '—Å–æ–≤–µ—Ç—Å–∫–∏–π', '—Å–æ–≤—Å–µ–º', '—Å–ø–∞—Å–∏–±–æ',
        '—Å–ø—Ä–æ—Å–∏—Ç—å', '—Å—Ä–∞–∑—É', '—Å—Ç–∞–ª', '—Å—Ç–∞—Ä—ã–π', '—Å—Ç–∞—Ç—å', '—Å—Ç–æ–ª', '—Å—Ç–æ—Ä–æ–Ω–∞',
        '—Å—Ç–æ—è—Ç—å', '—Å—Ç—Ä–∞–Ω–∞', '—Å—É—Ç—å', '—Å—á–∏—Ç–∞—Ç—å', '—Ç', '—Ç–∞', '—Ç–∞–∫', '—Ç–∞–∫–∞—è',
        '—Ç–∞–∫–∂–µ', '—Ç–∞–∫–∏', '—Ç–∞–∫–∏–µ', '—Ç–∞–∫–æ–µ', '—Ç–∞–∫–æ–π', '—Ç–∞–º', '—Ç–≤–æ–∏', '—Ç–≤–æ–π',
        '—Ç–≤–æ—è', '—Ç–≤–æ—ë', '—Ç–µ', '—Ç–µ–±–µ', '—Ç–µ–±—è', '—Ç–µ–º', '—Ç–µ–º–∏', '—Ç–µ–ø–µ—Ä—å',
        '—Ç–µ—Ö', '—Ç–æ', '—Ç–æ–±–æ–π', '—Ç–æ–±–æ—é', '—Ç–æ–≤–∞—Ä–∏—â', '—Ç–æ–≥–¥–∞', '—Ç–æ–≥–æ', '—Ç–æ–∂–µ',
        '—Ç–æ–ª—å–∫–æ', '—Ç–æ–º', '—Ç–æ–º—É', '—Ç–æ—Ç', '—Ç–æ—é', '—Ç—Ä–µ—Ç–∏–π', '—Ç—Ä–∏', '—Ç—Ä–∏–Ω–∞–¥—Ü–∞—Ç—ã–π',
        '—Ç—Ä–∏–Ω–∞–¥—Ü–∞—Ç—å', '—Ç—É', '—Ç—É–¥–∞', '—Ç—É—Ç', '—Ç—ã', '—Ç—ã—Å—è—á', '—É', '—É–≤–∏–¥–µ—Ç—å',
        '—É–∂', '—É–∂–µ', '—É–ª–∏—Ü–∞', '—É–º–µ—Ç—å', '—É—Ç—Ä–æ', '—Ö–æ—Ä–æ—à–∏–π', '—Ö–æ—Ä–æ—à–æ',
        '—Ö–æ—Ç–µ–ª –±—ã', '—Ö–æ—Ç–µ—Ç—å', '—Ö–æ—Ç—å', '—Ö–æ—Ç—è', '—Ö–æ—á–µ—à—å', '—á–∞—Å', '—á–∞—Å—Ç–æ',
        '—á–∞—Å—Ç—å', '—á–∞—â–µ', '—á–µ–≥–æ', '—á–µ–ª–æ–≤–µ–∫', '—á–µ–º', '—á–µ–º—É', '—á–µ—Ä–µ–∑', '—á–µ—Ç–≤–µ—Ä—Ç—ã–π',
        '—á–µ—Ç—ã—Ä–µ', '—á–µ—Ç—ã—Ä–Ω–∞–¥—Ü–∞—Ç—ã–π', '—á–µ—Ç—ã—Ä–Ω–∞–¥—Ü–∞—Ç—å', '—á—Ç–æ', '—á—Ç–æ–±', '—á—Ç–æ–±—ã',
        '—á—É—Ç—å', '—à–µ—Å—Ç–Ω–∞–¥—Ü–∞—Ç—ã–π', '—à–µ—Å—Ç–Ω–∞–¥—Ü–∞—Ç—å', '—à–µ—Å—Ç–æ–π', '—à–µ—Å—Ç—å', '—ç—Ç–∞',
        '—ç—Ç–∏', '—ç—Ç–∏–º', '—ç—Ç–∏–º–∏', '—ç—Ç–∏—Ö', '—ç—Ç–æ', '—ç—Ç–æ–≥–æ', '—ç—Ç–æ–π', '—ç—Ç–æ–º',
        '—ç—Ç–æ–º—É', '—ç—Ç–æ—Ç', '—ç—Ç—É', '—è', '—è–≤–ª—è—é—Å—å'
    }

    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
    def clean_text(text):
        text = str(text).lower()  # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = re.sub(r'[^–∞-—è–ê-–Øa-zA-Z\s]', '', text)  # –£–±–∏—Ä–∞–µ–º —Å–∏–º–≤–æ–ª—ã, –∫—Ä–æ–º–µ –±—É–∫–≤
        words = text.split()
        filtered_words = [word for word in words if word not in STOP_WORDS and len(word) > 2]
        return ' '.join(filtered_words)

    # –û—á–∏—â–∞–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã
    output_df['cleaned_text'] = output_df['text'].apply(clean_text)
    all_cleaned_text = ' '.join(output_df['cleaned_text'])

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±–ª–∞–∫–∞
    if not all_cleaned_text.strip():
        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π, —Å–æ–∑–¥–∞—ë–º –∑–∞–≥–ª—É—à–∫—É
        all_cleaned_text = "–ù–µ—Ç_–¥–∞–Ω–Ω—ã—Ö"

    # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç WordCloud
    wordcloud = WordCloud(
        width=1000,
        height=600,
        background_color='white',
        max_words=100
    ).generate(all_cleaned_text)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–ª–∞–∫–æ —Å–ª–æ–≤ –≤ —Ñ–∞–π–ª
    static_dir = '/workspaces/Pre-diploma-internship/pre-processing_service/polls/static/images'
    wordcloud_path = os.path.join(static_dir, 'wordcloud.png')
    wordcloud.to_file(wordcloud_path)
    
    # üî• –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –º–µ—Ç–∫—É –∫ URL, —á—Ç–æ–±—ã –æ—Ç–∫–ª—é—á–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
    timestamp = int(time.time())
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º URL —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º v={timestamp}
    wordcloud = '/static/images/wordcloud.png?v=' + str(timestamp)
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º URL-–∞–¥—Ä–µ—Å –æ–±–ª–∞–∫–∞ —Å–ª–æ–≤
    return wordcloud

@csrf_exempt
def upload_file(request):
    html_table = ''
    headers = []  # <-- –¥–æ–±–∞–≤–ª—è–µ–º –∑–¥–µ—Å—å
    table_data = []
    if request.method == 'POST':
        form = UploadFileForm(request.POST, request.FILES)
        if form.is_valid():
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
            up_file = form.save()

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            tokenizer, model = load_model_and_tokenizer()
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            model.to(device)
            model.eval()

            # –ß–∏—Ç–∞–µ–º CSV –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            file_path = default_storage.path(up_file.file.name)
            data = pd.read_csv(file_path, header=None)
            texts = data.iloc[:, 0].values

            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            dataset = PredictionDataset(texts=texts, tokenizer=tokenizer)
            dataloader = DataLoader(dataset, batch_size=64, shuffle=False)

            all_active_labels = []
            for batch in tqdm(dataloader, desc="–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"):
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                with torch.no_grad():
                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                    probs_batch = torch.sigmoid(outputs.logits).cpu().numpy()
                    probs_batch = probs_batch.squeeze(1) if probs_batch.ndim == 3 else probs_batch

                    adaptive_thresholds = np.max(probs_batch, axis=1) * SCALING_FACTOR
                    active_labels_batch = (probs_batch > adaptive_thresholds[:, np.newaxis]).astype(int)

                    active_indices_batch = [
                        ",".join(map(str, (np.where(active_labels)[0] + 1)))
                        if np.any(active_labels) else ""
                        for active_labels in active_labels_batch
                    ]
                all_active_labels.extend(active_indices_batch)
            
          
            # –§–æ—Ä–º–∏—Ä—É–µ–º DataFrame
            output_df = pd.DataFrame({
                "text": texts,
                "predicted_labels": all_active_labels
            })
            
            
            # –ü—É—Ç—å –∫ Excel
            labels_path = os.path.join(os.path.dirname(__file__), '–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.xlsx')

            if not os.path.exists(labels_path):
                raise FileNotFoundError(f"–§–∞–π–ª {labels_path} –Ω–µ –Ω–∞–π–¥–µ–Ω.")

            labels_df = pd.read_excel(labels_path)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
            required_columns = ['id', 'level_3']
            missing_cols = [col for col in required_columns if col not in labels_df.columns]
            if missing_cols:
                raise KeyError(f"–í Excel-—Ñ–∞–π–ª–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing_cols}")

            label_dict = dict(zip(labels_df['id'], labels_df['level_3']))

            def map_labels(label_str, label_map):
                try:
                    ids = [int(x.strip()) for x in str(label_str).split(",")]
                    return "; ".join([label_map[id] for id in ids if id in label_map])
                except Exception as e:
                    print("–û—à–∏–±–∫–∞ –ø—Ä–∏ –º–∞–ø–ø–∏–Ω–≥–µ –º–µ—Ç–æ–∫:", e)
                    return ""

            output_df['predicted_labels'] = output_df['predicted_labels'].apply(lambda x: map_labels(x, label_dict))
            
            

            # === –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV –≤ –ø–∞–º—è—Ç–∏ ===
            csv_buffer = StringIO()
            output_df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
            
            # –°–æ–∑–¥–∞–µ–º ContentFile –∏–∑ —Å—Ç—Ä–æ–∫–∏
            csv_content = ContentFile(csv_buffer.getvalue().encode('utf-8'))
            
            # –£–∫–∞–∑—ã–≤–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ (–º–æ–∂–Ω–æ –ª—é–±–æ–µ, –≥–ª–∞–≤–Ω–æ–µ ‚Äî —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º .csv)
            file_name = f"processed_{up_file.file.name.split('/')[-1]}.csv"
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª –≤ –ø–æ–ª–µ processed_file
            up_file.processed_file.save(file_name, csv_content)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –ë–î
            up_file.save()
            
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º preview (–ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫)
            preview = output_df.head(10).to_dict(orient='records')
    
            # –ü–æ–ª—É—á–∞–µ–º URL —Ñ–∞–π–ª–∞
            download_url = up_file.processed_file.url
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≥—Ä–∞—Ñ–∏–∫–∏
            graph1_url, graph2_url = generate_graphs(output_df)
            
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–±–ª–∞–∫–æ —Å–ª–æ–≤
            wordcloud_url = generate_wordcloud(output_df)
    
            return JsonResponse({
            'preview': preview,
            'download_url': download_url,
            'graph1_url': graph1_url,
            'graph2_url': graph2_url,
            'wordcloud_url': wordcloud_url
            
            })
            
        else:
            return JsonResponse({"error": form.errors}, status=400)
    else:
        form = UploadFileForm()
    categories = Category.objects.all()
    return render(request, 'polls/index.html', {'categories': categories, 'form': form, })