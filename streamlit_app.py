import streamlit as st
import pandas as pd
import numpy as np
import torch
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import requests
import re
import time
import os
import io


# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===
API_URL_UPLOAD = "http://localhost:8000/api/up_fles/"  # –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –Ω–∞ Django
API_URL_CATEGORIES = "http://localhost:8000/api/categorys/"
API_URL_SUBCATEGORIES = "http://localhost:8000/api/subcategorys/"

MAX_LENGTH = 150
SCALING_FACTOR = 0.7

# --- –°—Ç–æ–ø-—Å–ª–æ–≤–∞ ---
STOP_WORDS = {
        'c', '–∞', '–∞–ª–ª–æ', '–±–µ–∑', '–±–µ–ª—ã–π', '–±–ª–∏–∑–∫–æ', '–±–æ–ª–µ–µ', '–±–æ–ª—å—à–µ',
        '–±–æ–ª—å—à–æ–π', '–±—É–¥–µ–º', '–±—É–¥–µ—Ç', '–±—É–¥–µ—Ç–µ', '–±—É–¥–µ—à—å', '–±—É–¥—Ç–æ', '–±—É–¥—É',
        '–±—É–¥—É—Ç', '–±—É–¥—å', '–±—ã', '–±—ã–≤–∞–µ—Ç', '–±—ã–≤—å', '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–∏',
        '–±—ã–ª–æ', '–±—ã—Ç—å', '–≤', '–≤–∞–∂–Ω–∞—è', '–≤–∞–∂–Ω–æ–µ', '–≤–∞–∂–Ω—ã–µ', '–≤–∞–∂–Ω—ã–π', '–≤–∞–º',
        '–≤–∞–º–∏', '–≤–∞—Å', '–≤–∞—à', '–≤–∞—à–∞', '–≤–∞—à–µ', '–≤–∞—à–∏', '–≤–≤–µ—Ä—Ö', '–≤–¥–∞–ª–∏',
        '–≤–¥—Ä—É–≥', '–≤–µ–¥—å', '–≤–µ–∑–¥–µ', '–≤–µ—Ä–Ω—É—Ç—å—Å—è', '–≤–µ—Å—å', '–≤–µ—á–µ—Ä', '–≤–∑–≥–ª—è–¥',
        '–≤–∑—è—Ç—å', '–≤–∏–¥', '–≤–∏–¥–µ–ª', '–≤–∏–¥–µ—Ç—å', '–≤–º–µ—Å—Ç–µ', '–≤–Ω–µ', '–≤–Ω–∏–∑', '–≤–Ω–∏–∑—É',
        '–≤–æ', '–≤–æ–¥–∞', '–≤–æ–π–Ω–∞', '–≤–æ–∫—Ä—É–≥', '–≤–æ–Ω', '–≤–æ–æ–±—â–µ', '–≤–æ–ø—Ä–æ—Å',
        '–≤–æ—Å–µ–º–Ω–∞–¥—Ü–∞—Ç—ã–π', '–≤–æ—Å–µ–º–Ω–∞–¥—Ü–∞—Ç—å', '–≤–æ—Å–µ–º—å', '–≤–æ—Å—å–º–æ–π', '–≤–æ—Ç',
        '–≤–ø—Ä–æ—á–µ–º', '–≤—Ä–µ–º–µ–Ω–∏', '–≤—Ä–µ–º—è', '–≤—Å–µ', '–≤—Å–µ –µ—â–µ', '–≤—Å–µ–≥–¥–∞', '–≤—Å–µ–≥–æ',
        '–≤—Å–µ–º', '–≤—Å–µ–º–∏', '–≤—Å–µ–º—É', '–≤—Å–µ—Ö', '–≤—Å–µ—é', '–≤—Å—é', '–≤—Å—é–¥—É', '–≤—Å—è',
        '–≤—Å—ë', '–≤—Ç–æ—Ä–æ–π', '–≤—ã', '–≤—ã–π—Ç–∏', '–≥', '–≥–¥–µ', '–≥–ª–∞–≤–Ω—ã–π', '–≥–ª–∞–∑',
        '–≥–æ–≤–æ—Ä–∏–ª', '–≥–æ–≤–æ—Ä–∏—Ç', '–≥–æ–≤–æ—Ä–∏—Ç—å', '–≥–æ–¥', '–≥–æ–¥–∞', '–≥–æ–¥—É', '–≥–æ–ª–æ–≤–∞',
        '–≥–æ–ª–æ—Å', '–≥–æ—Ä–æ–¥', '–¥–∞', '–¥–∞–≤–∞—Ç—å', '–¥–∞–≤–Ω–æ', '–¥–∞–∂–µ', '–¥–∞–ª–µ–∫–∏–π',
        '–¥–∞–ª–µ–∫–æ', '–¥–∞–ª—å—à–µ', '–¥–∞—Ä–æ–º', '–¥–∞—Ç—å', '–¥–≤–∞', '–¥–≤–∞–¥—Ü–∞—Ç—ã–π', '–¥–≤–∞–¥—Ü–∞—Ç—å',
        '–¥–≤–µ', '–¥–≤–µ–Ω–∞–¥—Ü–∞—Ç—ã–π', '–¥–≤–µ–Ω–∞–¥—Ü–∞—Ç—å', '–¥–≤–µ—Ä—å', '–¥–≤—É—Ö', '–¥–µ–≤—è—Ç–Ω–∞–¥—Ü–∞—Ç—ã–π',
        '–¥–µ–≤—è—Ç–Ω–∞–¥—Ü–∞—Ç—å', '–¥–µ–≤—è—Ç—ã–π', '–¥–µ–≤—è—Ç—å', '–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ', '–¥–µ–ª', '–¥–µ–ª–∞–ª',
        '–¥–µ–ª–∞—Ç—å', '–¥–µ–ª–∞—é', '–¥–µ–ª–æ', '–¥–µ–Ω—å', '–¥–µ–Ω—å–≥–∏', '–¥–µ—Å—è—Ç—ã–π', '–¥–µ—Å—è—Ç—å',
        '–¥–ª—è', '–¥–æ', '–¥–æ–≤–æ–ª—å–Ω–æ', '–¥–æ–ª–≥–æ', '–¥–æ–ª–∂–µ–Ω', '–¥–æ–ª–∂–Ω–æ', '–¥–æ–ª–∂–Ω—ã–π',
        '–¥–æ–º', '–¥–æ—Ä–æ–≥–∞', '–¥—Ä—É–≥', '–¥—Ä—É–≥–∞—è', '–¥—Ä—É–≥–∏–µ', '–¥—Ä—É–≥–∏—Ö', '–¥—Ä—É–≥–æ',
        '–¥—Ä—É–≥–æ–µ', '–¥—Ä—É–≥–æ–π', '–¥—É–º–∞—Ç—å', '–¥—É—à–∞', '–µ', '–µ–≥–æ', '–µ–µ', '–µ–π', '–µ–º—É',
        '–µ—Å–ª–∏', '–µ—Å—Ç—å', '–µ—â–µ', '–µ—â—ë', '–µ—é', '–µ—ë', '–∂', '–∂–¥–∞—Ç—å', '–∂–µ', '–∂–µ–Ω–∞',
        '–∂–µ–Ω—â–∏–Ω–∞', '–∂–∏–∑–Ω—å', '–∂–∏—Ç—å', '–∑–∞', '–∑–∞–Ω—è—Ç', '–∑–∞–Ω—è—Ç–∞', '–∑–∞–Ω—è—Ç–æ',
        '–∑–∞–Ω—è—Ç—ã', '–∑–∞—Ç–µ–º', '–∑–∞—Ç–æ', '–∑–∞—á–µ–º', '–∑–¥–µ—Å—å', '–∑–µ–º–ª—è', '–∑–Ω–∞—Ç—å',
        '–∑–Ω–∞—á–∏—Ç', '–∑–Ω–∞—á–∏—Ç—å', '–∏', '–∏–¥–∏', '–∏–¥—Ç–∏', '–∏–∑', '–∏–ª–∏', '–∏–º', '–∏–º–µ–ª',
        '–∏–º–µ–Ω–Ω–æ', '–∏–º–µ—Ç—å', '–∏–º–∏', '–∏–º—è', '–∏–Ω–æ–≥–¥–∞', '–∏—Ö', '–∫', '–∫–∞–∂–¥–∞—è',
        '–∫–∞–∂–¥–æ–µ', '–∫–∞–∂–¥—ã–µ', '–∫–∞–∂–¥—ã–π', '–∫–∞–∂–µ—Ç—Å—è', '–∫–∞–∑–∞—Ç—å—Å—è', '–∫–∞–∫', '–∫–∞–∫–∞—è',
        '–∫–∞–∫–æ–π', '–∫–µ–º', '–∫–Ω–∏–≥–∞', '–∫–æ–≥–¥–∞', '–∫–æ–≥–æ', '–∫–æ–º', '–∫–æ–º–Ω–∞—Ç–∞', '–∫–æ–º—É',
        '–∫–æ–Ω–µ—Ü', '–∫–æ–Ω–µ—á–Ω–æ', '–∫–æ—Ç–æ—Ä–∞—è', '–∫–æ—Ç–æ—Ä–æ–≥–æ', '–∫–æ—Ç–æ—Ä–æ–π', '–∫–æ—Ç–æ—Ä—ã–µ',
        '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä—ã—Ö', '–∫—Ä–æ–º–µ', '–∫—Ä—É–≥–æ–º', '–∫—Ç–æ', '–∫—É–¥–∞', '–ª–µ–∂–∞—Ç—å',
        '–ª–µ—Ç', '–ª–∏', '–ª–∏—Ü–æ', '–ª–∏—à—å', '–ª—É—á—à–µ', '–ª—é–±–∏—Ç—å', '–ª—é–¥–∏', '–º',
        '–º–∞–ª–µ–Ω—å–∫–∏–π', '–º–∞–ª–æ', '–º–∞—Ç—å', '–º–∞—à–∏–Ω–∞', '–º–µ–∂–¥—É', '–º–µ–ª—è', '–º–µ–Ω–µ–µ',
        '–º–µ–Ω—å—à–µ', '–º–µ–Ω—è', '–º–µ—Å—Ç–æ', '–º–∏–ª–ª–∏–æ–Ω–æ–≤', '–º–∏–º–æ', '–º–∏–Ω—É—Ç–∞', '–º–∏—Ä',
        '–º–∏—Ä–∞', '–º–Ω–µ', '–º–Ω–æ–≥–æ', '–º–Ω–æ–≥–æ—á–∏—Å–ª–µ–Ω–Ω–∞—è', '–º–Ω–æ–≥–æ—á–∏—Å–ª–µ–Ω–Ω–æ–µ',
        '–º–Ω–æ–≥–æ—á–∏—Å–ª–µ–Ω–Ω—ã–µ', '–º–Ω–æ–≥–æ—á–∏—Å–ª–µ–Ω–Ω—ã–π', '–º–Ω–æ–π', '–º–Ω–æ—é', '–º–æ–≥', '–º–æ–≥—É',
        '–º–æ–≥—É—Ç', '–º–æ–∂', '–º–æ–∂–µ—Ç', '–º–æ–∂–µ—Ç –±—ã—Ç—å', '–º–æ–∂–Ω–æ', '–º–æ–∂—Ö–æ', '–º–æ–∏', '–º–æ–π',
        '–º–æ—Ä', '–º–æ—Å–∫–≤–∞', '–º–æ—á—å', '–º–æ—è', '–º–æ—ë', '–º—ã', '–Ω–∞', '–Ω–∞–≤–µ—Ä—Ö—É', '–Ω–∞–¥',
        '–Ω–∞–¥–æ', '–Ω–∞–∑–∞–¥', '–Ω–∞–∏–±–æ–ª–µ–µ', '–Ω–∞–π—Ç–∏', '–Ω–∞–∫–æ–Ω–µ—Ü', '–Ω–∞–º', '–Ω–∞–º–∏',
        '–Ω–∞—Ä–æ–¥', '–Ω–∞—Å', '–Ω–∞—á–∞–ª–∞', '–Ω–∞—á–∞—Ç—å', '–Ω–∞—à', '–Ω–∞—à–∞', '–Ω–∞—à–µ', '–Ω–∞—à–∏',
        '–Ω–µ', '–Ω–µ–≥–æ', '–Ω–µ–¥–∞–≤–Ω–æ', '–Ω–µ–¥–∞–ª–µ–∫–æ', '–Ω–µ–µ', '–Ω–µ–π', '–Ω–µ–∫–æ—Ç–æ—Ä—ã–π',
        '–Ω–µ–ª—å–∑—è', '–Ω–µ–º', '–Ω–µ–º–Ω–æ–≥–æ', '–Ω–µ–º—É', '–Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ', '–Ω–µ—Ä–µ–¥–∫–æ',
        '–Ω–µ—Å–∫–æ–ª—å–∫–æ', '–Ω–µ—Ç', '–Ω–µ—é', '–Ω–µ—ë', '–Ω–∏', '–Ω–∏–±—É–¥—å', '–Ω–∏–∂–µ', '–Ω–∏–∑–∫–æ',
        '–Ω–∏–∫–∞–∫–æ–π', '–Ω–∏–∫–æ–≥–¥–∞', '–Ω–∏–∫—Ç–æ', '–Ω–∏–∫—É–¥–∞', '–Ω–∏–º', '–Ω–∏–º–∏', '–Ω–∏—Ö',
        '–Ω–∏—á–µ–≥–æ', '–Ω–∏—á—Ç–æ', '–Ω–æ', '–Ω–æ–≤—ã–π', '–Ω–æ–≥–∞', '–Ω–æ—á—å', '–Ω—É', '–Ω—É–∂–Ω–æ',
        '–Ω—É–∂–Ω—ã–π', '–Ω—Ö', '–æ', '–æ–±', '–æ–±–∞', '–æ–±—ã—á–Ω–æ', '–æ–¥–∏–Ω', '–æ–¥–∏–Ω–Ω–∞–¥—Ü–∞—Ç—ã–π',
        '–æ–¥–∏–Ω–Ω–∞–¥—Ü–∞—Ç—å', '–æ–¥–Ω–∞–∂–¥—ã', '–æ–¥–Ω–∞–∫–æ', '–æ–¥–Ω–æ–≥–æ', '–æ–¥–Ω–æ–π', '–æ–∫–∞–∑–∞—Ç—å—Å—è',
        '–æ–∫–Ω–æ', '–æ–∫–æ–ª–æ', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–∏', '–æ–Ω–æ', '–æ–ø—è—Ç—å', '–æ—Å–æ–±–µ–Ω–Ω–æ',
        '–æ—Å—Ç–∞—Ç—å—Å—è', '–æ—Ç', '–æ—Ç–≤–µ—Ç–∏—Ç—å', '–æ—Ç–µ—Ü', '–æ—Ç–∫—É–¥–∞', '–æ—Ç–æ–≤—Å—é–¥—É', '–æ—Ç—Å—é–¥–∞',
        '–æ—á–µ–Ω—å', '–ø–µ—Ä–≤—ã–π', '–ø–µ—Ä–µ–¥', '–ø–∏—Å–∞—Ç—å', '–ø–ª–µ—á–æ', '–ø–æ', '–ø–æ–¥', '–ø–æ–¥–æ–π–¥–∏',
        '–ø–æ–¥—É–º–∞—Ç—å', '–ø–æ–∂–∞–ª—É–π—Å—Ç–∞', '–ø–æ–∑–∂–µ', '–ø–æ–π—Ç–∏', '–ø–æ–∫–∞', '–ø–æ–ª', '–ø–æ–ª—É—á–∏—Ç—å',
        '–ø–æ–º–Ω–∏—Ç—å', '–ø–æ–Ω–∏–º–∞—Ç—å', '–ø–æ–Ω—è—Ç—å', '–ø–æ—Ä', '–ø–æ—Ä–∞', '–ø–æ—Å–ª–µ', '–ø–æ—Å–ª–µ–¥–Ω–∏–π',
        '–ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å', '–ø–æ—Å—Ä–µ–¥–∏', '–ø–æ—Ç–æ–º', '–ø–æ—Ç–æ–º—É', '–ø–æ—á–µ–º—É', '–ø–æ—á—Ç–∏',
        '–ø—Ä–∞–≤–¥–∞', '–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ', '–ø—Ä–∏', '–ø—Ä–æ', '–ø—Ä–æ—Å—Ç–æ', '–ø—Ä–æ—Ç–∏–≤', '–ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤',
        '–ø—É—Ç—å', '–ø—è—Ç–Ω–∞–¥—Ü–∞—Ç—ã–π', '–ø—è—Ç–Ω–∞–¥—Ü–∞—Ç—å', '–ø—è—Ç—ã–π', '–ø—è—Ç—å', '—Ä–∞–±–æ—Ç–∞',
        '—Ä–∞–±–æ—Ç–∞—Ç—å', '—Ä–∞–∑', '—Ä–∞–∑–≤–µ', '—Ä–∞–Ω–æ', '—Ä–∞–Ω—å—à–µ', '—Ä–µ–±–µ–Ω–æ–∫', '—Ä–µ—à–∏—Ç—å',
        '—Ä–æ—Å—Å–∏—è', '—Ä—É–∫–∞', '—Ä—É—Å—Å–∫–∏–π', '—Ä—è–¥', '—Ä—è–¥–æ–º', '—Å', '—Å –∫–µ–º', '—Å–∞–º',
        '—Å–∞–º–∞', '—Å–∞–º–∏', '—Å–∞–º–∏–º', '—Å–∞–º–∏–º–∏', '—Å–∞–º–∏—Ö', '—Å–∞–º–æ', '—Å–∞–º–æ–≥–æ',
        '—Å–∞–º–æ–π', '—Å–∞–º–æ–º', '—Å–∞–º–æ–º—É', '—Å–∞–º—É', '—Å–∞–º—ã–π', '—Å–≤–µ—Ç', '—Å–≤–æ–µ', '—Å–≤–æ–µ–≥–æ',
        '—Å–≤–æ–µ–π', '—Å–≤–æ–∏', '—Å–≤–æ–∏—Ö', '—Å–≤–æ–π', '—Å–≤–æ—é', '—Å–¥–µ–ª–∞—Ç—å', '—Å–µ–∞–æ–π', '—Å–µ–±–µ',
        '—Å–µ–±—è', '—Å–µ–≥–æ–¥–Ω—è', '—Å–µ–¥—å–º–æ–π', '—Å–µ–π—á–∞—Å', '—Å–µ–º–Ω–∞–¥—Ü–∞—Ç—ã–π', '—Å–µ–º–Ω–∞–¥—Ü–∞—Ç—å',
        '—Å–µ–º—å', '—Å–∏–¥–µ—Ç—å', '—Å–∏–ª–∞', '—Å–∏—Ö', '—Å–∫–∞–∑–∞–ª', '—Å–∫–∞–∑–∞–ª–∞', '—Å–∫–∞–∑–∞—Ç—å',
        '—Å–∫–æ–ª—å–∫–æ', '—Å–ª–∏—à–∫–æ–º', '—Å–ª–æ–≤–æ', '—Å–ª—É—á–∞–π', '—Å–º–æ—Ç—Ä–µ—Ç—å', '—Å–Ω–∞—á–∞–ª–∞',
        '—Å–Ω–æ–≤–∞', '—Å–æ', '—Å–æ–±–æ–π', '—Å–æ–±–æ—é', '—Å–æ–≤–µ—Ç—Å–∫–∏–π', '—Å–æ–≤—Å–µ–º', '—Å–ø–∞—Å–∏–±–æ',
        '—Å–ø—Ä–æ—Å–∏—Ç—å', '—Å—Ä–∞–∑—É', '—Å—Ç–∞–ª', '—Å—Ç–∞—Ä—ã–π', '—Å—Ç–∞—Ç—å', '—Å—Ç–æ–ª', '—Å—Ç–æ—Ä–æ–Ω–∞',
        '—Å—Ç–æ—è—Ç—å', '—Å—Ç—Ä–∞–Ω–∞', '—Å—É—Ç—å', '—Å—á–∏—Ç–∞—Ç—å', '—Ç', '—Ç–∞', '—Ç–∞–∫', '—Ç–∞–∫–∞—è',
        '—Ç–∞–∫–∂–µ', '—Ç–∞–∫–∏', '—Ç–∞–∫–∏–µ', '—Ç–∞–∫–æ–µ', '—Ç–∞–∫–æ–π', '—Ç–∞–º', '—Ç–≤–æ–∏', '—Ç–≤–æ–π',
        '—Ç–≤–æ—è', '—Ç–≤–æ—ë', '—Ç–µ', '—Ç–µ–±–µ', '—Ç–µ–±—è', '—Ç–µ–º', '—Ç–µ–º–∏', '—Ç–µ–ø–µ—Ä—å',
        '—Ç–µ—Ö', '—Ç–æ', '—Ç–æ–±–æ–π', '—Ç–æ–±–æ—é', '—Ç–æ–≤–∞—Ä–∏—â', '—Ç–æ–≥–¥–∞', '—Ç–æ–≥–æ', '—Ç–æ–∂–µ',
        '—Ç–æ–ª—å–∫–æ', '—Ç–æ–º', '—Ç–æ–º—É', '—Ç–æ—Ç', '—Ç–æ—é', '—Ç—Ä–µ—Ç–∏–π', '—Ç—Ä–∏', '—Ç—Ä–∏–Ω–∞–¥—Ü–∞—Ç—ã–π',
        '—Ç—Ä–∏–Ω–∞–¥—Ü–∞—Ç—å', '—Ç—É', '—Ç—É–¥–∞', '—Ç—É—Ç', '—Ç—ã', '—Ç—ã—Å—è—á', '—É', '—É–≤–∏–¥–µ—Ç—å',
        '—É–∂', '—É–∂–µ', '—É–ª–∏—Ü–∞', '—É–º–µ—Ç—å', '—É—Ç—Ä–æ', '—Ö–æ—Ä–æ—à–∏–π', '—Ö–æ—Ä–æ—à–æ',
        '—Ö–æ—Ç–µ–ª –±—ã', '—Ö–æ—Ç–µ—Ç—å', '—Ö–æ—Ç—å', '—Ö–æ—Ç—è', '—Ö–æ—á–µ—à—å', '—á–∞—Å', '—á–∞—Å—Ç–æ',
        '—á–∞—Å—Ç—å', '—á–∞—â–µ', '—á–µ–≥–æ', '—á–µ–ª–æ–≤–µ–∫', '—á–µ–º', '—á–µ–º—É', '—á–µ—Ä–µ–∑', '—á–µ—Ç–≤–µ—Ä—Ç—ã–π',
        '—á–µ—Ç—ã—Ä–µ', '—á–µ—Ç—ã—Ä–Ω–∞–¥—Ü–∞—Ç—ã–π', '—á–µ—Ç—ã—Ä–Ω–∞–¥—Ü–∞—Ç—å', '—á—Ç–æ', '—á—Ç–æ–±', '—á—Ç–æ–±—ã',
        '—á—É—Ç—å', '—à–µ—Å—Ç–Ω–∞–¥—Ü–∞—Ç—ã–π', '—à–µ—Å—Ç–Ω–∞–¥—Ü–∞—Ç—å', '—à–µ—Å—Ç–æ–π', '—à–µ—Å—Ç—å', '—ç—Ç–∞',
        '—ç—Ç–∏', '—ç—Ç–∏–º', '—ç—Ç–∏–º–∏', '—ç—Ç–∏—Ö', '—ç—Ç–æ', '—ç—Ç–æ–≥–æ', '—ç—Ç–æ–π', '—ç—Ç–æ–º',
        '—ç—Ç–æ–º—É', '—ç—Ç–æ—Ç', '—ç—Ç—É', '—è', '—è–≤–ª—è—é—Å—å'
    }

# --- –ú–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä ---
@st.cache_resource
def load_model_and_tokenizer(model_path):
    tokenizer = BertTokenizer.from_pretrained(model_path)
    model = BertForSequenceClassification.from_pretrained(model_path)
    return tokenizer, model

class PredictionDataset(Dataset):
    def __init__(self, texts, tokenizer):
        self.texts = texts
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        if not isinstance(text, str):
            text = str(text)
        encoding = self.tokenizer(
            text,
            padding="max_length",
            truncation=True,
            max_length=MAX_LENGTH,
            return_tensors="pt"
        )
        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
        }

# --- –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–µ—Ç–æ–∫ ---
def predict_labels(texts, model_path="/workspaces/Pre-diploma-internship-micro/best_model"):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    tokenizer, model = load_model_and_tokenizer(model_path)
    model.to(device)
    model.eval()

    dataset = PredictionDataset(texts=texts, tokenizer=tokenizer)
    dataloader = DataLoader(dataset, batch_size=64, shuffle=False)

    all_active_labels = []

    for batch in dataloader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        with torch.no_grad():
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            probs_batch = torch.sigmoid(outputs.logits).cpu().numpy()
            probs_batch = probs_batch.squeeze(1) if probs_batch.ndim == 3 else probs_batch
            adaptive_thresholds = np.max(probs_batch, axis=1) * SCALING_FACTOR
            active_labels_batch = (probs_batch > adaptive_thresholds[:, np.newaxis]).astype(int)
            active_indices_batch = [
                ",".join(map(str, (np.where(active_labels)[0] + 1)))
                if np.any(active_labels) else ""
                for active_labels in active_labels_batch
            ]
        all_active_labels.extend(active_indices_batch)

    return all_active_labels

#  –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞ –≤ –±–∞–π—Ç—ã
def fig_to_bytes(fig):
    buf = io.BytesIO()
    fig.savefig(buf, format="png", bbox_inches='tight', dpi=100)
    plt.close(fig)
    buf.seek(0)
    return buf

# --- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ ---
def generate_graphs(output_df):
    labels_path = "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.xlsx"
    labels_df = pd.read_excel(labels_path)
    label_to_category = dict(zip(labels_df['level_3'], labels_df['level_2']))

    def map_to_category(labels_str):
        if not isinstance(labels_str, str) or not labels_str.strip():
            return []
        labels = [label.strip() for label in labels_str.split(";")]
        categories = set(label_to_category[label] for label in labels if label in label_to_category)
        return list(categories)

    output_df['categories'] = output_df['predicted_labels'].apply(map_to_category)
    exploded_categories = output_df.explode('categories')
    category_counts = exploded_categories['categories'].value_counts().sort_values(ascending=False)
    sorted_categories = category_counts.sort_values(ascending=True)

    colors = ['#fd7e14', '#6c757d', '#17a2b8', '#dc3545', '#ffc107', '#28a745', '#007bff']

    # === –ì—Ä–∞—Ñ–∏–∫ 1: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º ===
    fig1, ax1 = plt.subplots(figsize=(12, 6))
    bars = ax1.barh(sorted_categories.index, sorted_categories.values, color=colors[:len(sorted_categories)], height=0.5)
    ax1.grid(axis='x', color='gray', linestyle='--', linewidth=0.7, zorder=1)
    ax1.set_facecolor('none')

    graph1_buffer = fig_to_bytes(fig1)

    # === –ì—Ä–∞—Ñ–∏–∫ 2: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è–º ===
    subcategory_counts = output_df['predicted_labels'].str.split('; ').explode().value_counts().head(10)

    fig2, ax2 = plt.subplots(figsize=(12, 6))
    bars = ax2.barh(subcategory_counts.index[::-1], subcategory_counts.values[::-1], color='#BA68C8', height=0.5)
    ax2.grid(axis='x', color='gray', linestyle='--', linewidth=0.7, zorder=1)
    ax2.set_facecolor('none')

    graph2_buffer = fig_to_bytes(fig2)

    return graph1_buffer, graph2_buffer

# --- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±–ª–∞–∫–∞ —Å–ª–æ–≤ ---
def generate_wordcloud(output_df):
    def clean_text(text):
        text = str(text).lower()
        text = re.sub(r'[^–∞-—è–ê-–Øa-zA-Z\s]', '', text)
        words = text.split()
        filtered_words = [word for word in words if word not in STOP_WORDS and len(word) > 2]
        return ' '.join(filtered_words)

    output_df['cleaned_text'] = output_df['text'].apply(clean_text)
    all_cleaned_text = ' '.join(output_df['cleaned_text'])

    if not all_cleaned_text.strip():
        all_cleaned_text = "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"

    wordcloud = WordCloud(width=1500, height=800, background_color='white', max_words=100).generate(all_cleaned_text)

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º WordCloud –≤ BytesIO
    wordcloud_buffer = io.BytesIO()
    wordcloud.to_image().save(wordcloud_buffer, format="PNG")
    wordcloud_buffer.seek(0)

    return wordcloud_buffer

# --- –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ ---
def process_uploaded_file(df):
    texts = df.iloc[:, 0].values
    predicted_indices = predict_labels(texts)

    output_df = pd.DataFrame({"text": texts, "predicted_labels": predicted_indices})

    labels_df = pd.read_excel("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.xlsx")
    label_dict = dict(zip(labels_df['id'], labels_df['level_3']))

    def map_labels(label_str, label_map):
        try:
            ids = [int(x.strip()) for x in str(label_str).split(",")]
            return "; ".join([label_map[id] for id in ids if id in label_map])
        except Exception as e:
            return ""

    output_df['predicted_labels'] = output_df['predicted_labels'].apply(lambda x: map_labels(x, label_dict))

    output_df = output_df[['text', 'predicted_labels']]
    return output_df



# === UI ===
st.set_page_config(page_title="–°–µ—Ä–≤–∏—Å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π", layout="wide")
st.title("–°–µ—Ä–≤–∏—Å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π")

# === –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏ –∏ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏ ===
with st.sidebar:
    st.header("–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ")
    st.markdown("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö:")
    search_term = st.text_input("–ü–æ–∏—Å–∫ –ø–æ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è–º")

    try:
        response = requests.get(API_URL_CATEGORIES)
        if response.status_code == 200:
            categories = response.json()
        else:
            categories = []
            st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏")
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ API: {e}")
        categories = []

    for category in categories:
        with st.expander(f"{category['emoji']} {category['name']}"):
            try:
                subcat_response = requests.get(
                    f"{API_URL_SUBCATEGORIES}?category={category['identifier']}"
                )
                if subcat_response.status_code == 200:
                    subcategories = subcat_response.json()
                else:
                    subcategories = []
            except:
                subcategories = []

            filtered_subcats = [
                s for s in subcategories
                if search_term.lower() in s["name"].lower() or not search_term
            ]

            if not filtered_subcats:
                st.markdown("*–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ*")
            else:
                for subcat in filtered_subcats:
                    with st.popover(subcat["name"]):
                        st.markdown(subcat["description"])

# === –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç ===
st.markdown("## –ö–∞–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Å–µ—Ä–≤–∏—Å–æ–º:")
st.markdown("""
1Ô∏è‚É£ **–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª**  
   –ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É ¬´–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª¬ª –∏ –≤—ã–±–µ—Ä–∏—Ç–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–π —Ñ–∞–π–ª –≤ —Ñ–æ—Ä–º–∞—Ç–µ `.csv`. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ.

2Ô∏è‚É£ **–î–æ–∂–¥–∏—Ç–µ—Å—å –æ–±—Ä–∞–±–æ—Ç–∫–∏**  
   –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–∞—á–Ω—ë—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö. –ü—Ä–æ–≥—Ä–µ—Å—Å –±—É–¥–µ—Ç –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å—Å—è –Ω–∞ —ç–∫—Ä–∞–Ω–µ.

3Ô∏è‚É£ **–ü—Ä–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –∏ —Å–∫–∞—á–∞–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã**  
   –ü–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –≤—ã —É–≤–∏–¥–∏—Ç–µ —Ç–∞–±–ª–∏—Ü—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞. –¢–∞–∫–∂–µ —Å—Ç–∞–Ω–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π CSV-—Ñ–∞–π–ª.
""")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è session_state
if 'processed_df' not in st.session_state:
    st.session_state.processed_df = None
    
st.markdown("### –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ CSV-—Ñ–∞–π–ª", type=["csv"], label_visibility="collapsed")

if uploaded_file is not None:
    if st.button("–ù–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É"):
        try:
            df = pd.read_csv(uploaded_file)
            status_text = st.empty()
            progress_bar = st.progress(0)

            status_text.info("–ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏...")
            processed_df = process_uploaded_file(df)

            status_text.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤...")
            wordcloud_buffer = generate_wordcloud(processed_df)
            graph1_buffer, graph2_buffer = generate_graphs(processed_df)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å—ë –≤ session_state, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞—Ç—å –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏
            st.session_state.processed_df = processed_df
            st.session_state.wordcloud_buffer = wordcloud_buffer
            st.session_state.graph1_buffer = graph1_buffer
            st.session_state.graph2_buffer = graph2_buffer

            status_text.success("–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω!")
            progress_bar.progress(100)

        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞: {e}")

# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –µ—Å–ª–∏ –æ–Ω–∏ —É–∂–µ –µ—Å—Ç—å –≤ session_state
if st.session_state.processed_df is not None:
    processed_df = st.session_state.processed_df
    


    # –ü—Ä–∏–º–µ—Ä –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Å–ø–∏—Å–∫–∞ –º–µ—Ç–æ–∫ –≤ HTML-–±—ç–π–¥–∂–∏
    def labels_to_badges(labels):
        if isinstance(labels, str):
            labels = labels.split(';')  # –µ—Å–ª–∏ –º–µ—Ç–∫–∏ –∑–∞–ø–∏—Å–∞–Ω—ã –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é
        badges = ''.join(
            [f'<span style="margin:2px 4px; padding:4px 8px; background-color:#f0f2f6; border-radius:12px; display:inline-block; min-width:100px; text-align:left;">{label}</span>' 
            for label in labels]
        )
        return badges

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ DataFrame –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    display_df = processed_df[['text', 'predicted_labels']].head(10).copy()
    display_df['predicted_labels'] = display_df['predicted_labels'].apply(labels_to_badges)
    display_df.columns = ['–¢–µ–∫—Å—Ç', '–ú–µ—Ç–∫–∏']

    # –°—Ç–∏–ª–∏ –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã
    st.markdown("""
        <style>
        .badge-table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 10px;
        }
        .badge-table th, .badge-table td {
            padding: 10px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }
        .badge-table tr:hover {
            background-color: #f9f9f9;
        }

        .st-emotion-cache-16tyu1 th{
            text-align: left;
        }

        td {
            vertical-align: top;
        }
        </style>
    """, unsafe_allow_html=True)

    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è DataFrame –≤ HTML –∏ –≤—ã–≤–æ–¥
    html_table = display_df.to_html(index=False, escape=False)
    st.markdown(f'<table class="badge-table">{html_table}</table>', unsafe_allow_html=True)


    st.markdown("## –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫):")
    st.dataframe(processed_df[['text', 'predicted_labels']].head(10))

    # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ CSV
    csv_ready_df = processed_df[['text', 'predicted_labels']]
    csv_data = csv_ready_df.to_csv(index=False).encode()
    st.download_button(
        label="–°–∫–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ñ–∞–π–ª",
        data=csv_data,
        file_name="processed_data.csv",
        mime="text/csv"
    )

    # –ì—Ä–∞—Ñ–∏–∫–∏
    if st.session_state.wordcloud_buffer:
        st.markdown("#### ‚òÅÔ∏è –û–±–ª–∞–∫–æ —Å–ª–æ–≤")
        st.image(st.session_state.wordcloud_buffer, use_container_width=True)

    if st.session_state.graph1_buffer:
        st.markdown("#### üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º")
        st.image(st.session_state.graph1_buffer, use_container_width=True)

    if st.session_state.graph2_buffer:
        st.markdown("#### üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è–º")
        st.image(st.session_state.graph2_buffer, use_container_width=True)